\section{Derivation of the Adjoint Method for Neural ODEs}
\label{app:adjoint_method}

\textit{This appendix provides a detailed derivation of the adjoint method as presented in Appendix B of the paper 
\cite{chen2019neuralordinarydifferentialequations}. 
The derivation here is presented in a more intuitive manner, aiming to deduce the result from 
first principles rather than starting with the definition of the adjoint state.}

\subsection{Context}

In a standard Recurrent Neural Network (RNN), a sequence of hidden states is generated, 
where each state carries information to the next:
\begin{equation}
    \vec{h}_{t+1} = \vec{h}_t + f(\vec{h}_t, \theta_t)
\end{equation}

The Neural Ordinary Differential Equation (NODE) model extends this concept to the continuous-time domain. 
As the time steps approach zero, the sequence of hidden states can be modeled as a continuous trajectory,
$\vec{z}(t)$, governed by an ordinary differential equation (ODE):
\begin{equation}
    \frac{d\vec{z}(t)}{dt} = \dot{\vec{z}} = f(\vec{z}(t), \theta, t)
    \label{eq:node_dynamics}
\end{equation}

We wish to find the gradient of a loss function $L$ with respect to the parameters $\theta$. 
Typically, $L$ depends directly only on the state at the final time, $L(\vec{z}(t_1))$. 
To find this gradient, we use the method of Lagrange multipliers to construct a new functional, $\mathcal{J}$, 
which incorporates the dynamic constraint of the ODE.

\begin{equation}
    \mathcal{J}(\vec{z}, \vec{\lambda}, \theta) =
    L(\vec{z}(t_1)) +
    \int_{t_0}^{t_1} \vec{\lambda}^{T}(t)
    \left[
    f(\vec{z}(t), \theta, t) - \dot{\vec{z}}(t)
    \right]
    \ dt
    \label{eq:adjoint_functional}
\end{equation}

By construction, the value of $\mathcal{J}$ is identical to $L$ whenever the dynamic constraint is satisfied. 
We analyze the first variation of $\mathcal{J}$, denoted $\delta\mathcal{J}$, by considering the contributions from each of its arguments:
\begin{equation}
    \delta \mathcal{J} =
    \frac{\delta \mathcal{J}}{\delta \vec{z}}\ \delta \vec{z} +
    \frac{\delta \mathcal{J}}{\delta \theta}\ \delta \theta +
    \frac{\delta \mathcal{J}}{\delta \vec{\lambda}}\ \delta \vec{\lambda}
\end{equation}

\subsection{First Variation of the Functional}

\subsubsection{Variation with Respect to $\vec{\lambda}$}

Setting the variation with respect to the Lagrange multiplier $\vec{\lambda}$ to zero recovers the original dynamic constraint:
\begin{equation}
    \frac{\delta \mathcal{J}}{\delta \vec{\lambda}} = f(\vec{z}, \theta, t) - \dot{\vec{z}} = 0 \quad \implies \quad \dot{\vec{z}} = f(\vec{z}, \theta, t)
\end{equation}

\subsubsection{Variation with Respect to $\theta$}

The contribution from the explicit dependence on the parameter $\theta$ is:
\begin{equation}
    \frac{\delta \mathcal{J}}{\delta \theta}\ \delta \theta =
    \int_{t_0}^{t_1} \vec{\lambda}^{T}(t)\
        \frac{\partial f}{\partial \theta}\ \delta \theta\ dt
\end{equation}

\subsubsection{Variation with Respect to $\vec{z}$}

The variation of $\mathcal{J}$ with respect to the trajectory $\vec{z}(t)$ is:
\begin{equation}
    \frac{\delta \mathcal{J}}{\delta \vec{z}}\ \delta \vec{z} =
    \frac{\partial L}{\partial \vec{z}(t_1)}\ \delta \vec{z}(t_1) +
    \int_{t_0}^{t_1} \vec{\lambda}^{T}(t)\left[
        \frac{\partial f}{\partial \vec{z}}
        \ \delta \vec{z} - \delta \dot{\vec{z}}\
    \right]\ dt
\end{equation}

We apply integration by parts to the term containing $\delta \dot{\vec{z}}$:
\begin{equation}
    -\int_{t_0}^{t_1} \vec{\lambda}^T \delta \dot{\vec{z}} \ dt =
    \int_{t_0}^{t_1} \dot{\vec{\lambda}}^T \delta\vec{z} \ dt -
    \left[\ \vec{\lambda}^T \delta\vec{z}\ \right]_{t_0}^{t_1}
\end{equation}

Substituting this back and rearranging terms yields:
\begin{equation}
    \begin{split}
        \frac{\delta \mathcal{J}}{\delta \vec{z}}\ \delta \vec{z} =
        & \left(
            \frac{\partial L}{\partial \vec{z}(t_1)} - \vec{\lambda}^T(t_1)
        \right) \delta\vec{z}(t_1) \\
        & + \vec{\lambda}^T(t_0)\ \delta\vec{z}(t_0) +
        \int_{t_0}^{t_1} \left[
            \vec{\lambda}^{T}\ \frac{\partial f}{\partial \vec{z}} +
            \dot{\vec{\lambda}}^{T}\
        \right]\delta \vec{z} \ dt
    \end{split}
\end{equation}

Since the initial condition $\vec{z}(t_0)$ is fixed, its variation $\delta\vec{z}(t_0)$ is zero. 
To nullify the remaining arbitrary variations, 
we strategically define the \textbf{adjoint state} $\vec{\lambda}(t)$ by enforcing two conditions:
\begin{enumerate}
    \item \textbf{Boundary Condition at Final Time ($t_1$):}
    \begin{equation}
        \vec{\lambda}^T(t_1) = \frac{\partial L}{\partial \vec{z}(t_1)}
        \label{eq:adjoint_bvp}
    \end{equation}
    \item \textbf{Differential Equation of the Adjoint State:}
    \begin{equation}
        \dot{\vec{\lambda}}^{T}(t) = - \vec{\lambda}^{T}(t)\ \frac{\partial f}{\partial \vec{z}}
        \label{eq:adjoint_dynamics}
    \end{equation}
\end{enumerate}

By imposing these conditions, the entire variation with respect to $\vec{z}$ vanishes.

\subsection{Final Result}
With the variations with respect to $\vec{z}$ and $\vec{\lambda}$ being zero, the total variation 
$\delta L = \delta \mathcal{J}$ simplifies to only the explicit contribution from $\theta$:
\begin{equation}
    \frac{d L}{d \theta}\ \delta \theta =
    \int_{t_0}^{t_1} \vec{\lambda}^{T}(t)\
        \frac{\partial f}{\partial \theta}\ \delta \theta\ dt
\end{equation}

This gives the final expression for the gradient of the loss function:
\begin{equation}
    \frac{d L}{d \theta} =
    \int_{t_0}^{t_1} \vec{\lambda}^{T}(t)\
        \frac{\partial f}{\partial \theta}\ dt
    \label{eq:adjoint_grad}
\end{equation}